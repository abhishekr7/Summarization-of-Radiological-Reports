{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input, GRU \n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "class AttentionGRU(Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 output_dim, \n",
    "                 name = 'AttentionGRU', \n",
    "                 activation = 'tanh',\n",
    "                 return_probabilities = False,\n",
    "                 kernel_initializer = 'glorot_uniform', \n",
    "                 recurrent_initializer = 'orthogonal', \n",
    "                 bias_initializer = 'zeros', \n",
    "                 kernel_regularizer = None,\n",
    "                 recurrent_regularizer = None,\n",
    "                 activity_regularizer = None,\n",
    "                 bias_regularizer = None,\n",
    "                 kernel_constraint = None,\n",
    "                 recurrent_constraint = None,\n",
    "                 bias_constraint = None,\n",
    "                 **kwargs):\n",
    "        \n",
    "        #define parameters\n",
    "        #called when layer gets instantiated \n",
    "        \n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activations.get(activation)\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizers)\n",
    "        \n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        \n",
    "        self.name = name\n",
    "        self.return_sequences = True     #redundant.. but okay!\n",
    "        \n",
    "        super(AttentionGRU, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #is called when we run Model.compile(...)\n",
    "        \n",
    "        # define weights here\n",
    "        \n",
    "        #self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "        \n",
    "        #if self.stateful:\n",
    "        #   super(AttentionLayer).reset_states()\n",
    "        \n",
    "        #self.states = [None, None]    # y, s\n",
    "        \n",
    "        \"\"\"\"\"\n",
    "            Learnable Weights(?) for the \"pre-softmax\" attention vector \n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        \n",
    "        self.Mu_T = self.add_weight(shape = (self.units,), \n",
    "                                   name = 'Mu_T',\n",
    "                                   initializer = self.kernel_initializer,\n",
    "                                   regularizer = self.kernel_regularizer, \n",
    "                                   constraint = self.kernel_constraint)\n",
    "        \n",
    "        self.W_h = self.add_weight(shape = (self.input_dim, self.units),\n",
    "                                  name = 'W_h',\n",
    "                                  initializer = self.kernel_initializer,\n",
    "                                  regularizer = self.kernel_regularizer,\n",
    "                                  constraint = self.kernel_constraint)\n",
    "        \n",
    "        self.W_s = self.add_weight(shape = (self.units, self.units),\n",
    "                                  name = 'W_s',\n",
    "                                  initializer = self.kernel_initializer,\n",
    "                                  regularizer = self.kernel_regularizer,\n",
    "                                  constraint = self.kernel_constraint)\n",
    "        \n",
    "        #bias to be added or not\n",
    "        self.b_a = self.add_weight(shape = (self.units),\n",
    "                                  name = 'b_a',\n",
    "                                  initializer = self.bias_initializer,\n",
    "                                  regularizer = self.bias_regularizer,\n",
    "                                  constraint = self.bias_constraint)\n",
    "        \n",
    "        \"\"\"\"\"\n",
    "            Learnable weights for the update (z) gate\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        self.W_z = self.add_weight(shape = (self.output_dim, self.units),\n",
    "                                  name = 'W_z',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.U_z = self.add_weight(shape = (self.units, self.units),\n",
    "                                  name = 'U_z',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.b_z = self.add_weight(shape = (self.units,),\n",
    "                                  name = 'b_z',\n",
    "                                  initializer = self.bias_initializer,\n",
    "                                  regularizer = self.bias_regularizer,\n",
    "                                  constraint = self.bias_constraint)\n",
    "        \n",
    "        \"\"\"\"\"\n",
    "            Learnable Weights for the reset (r) gate\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        self.W_r = self.add_weight(shape = (self.output_dim, self.units),\n",
    "                                  name = 'W_r',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.U_r = self.add_weight(shape = (self.units, self.units),\n",
    "                                  name = 'U_r',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.b_r = self.add_weight(shape = (self.units,),\n",
    "                                  name = 'b_r',\n",
    "                                  initializer = self.bias_initializer,\n",
    "                                  regularizer = self.bias_regularizer,\n",
    "                                  constraint = self.bias_constraint)\n",
    "        \n",
    "        \"\"\"\"\"\n",
    "            Learnable Weights for the intermediate(c) gate\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        self.W_c = self.add_weight(shape = (self.output_dim, self.units),\n",
    "                                  name = 'W_c',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.U_c = self.add_weight(shape = (self.units, self.units),\n",
    "                                  name = 'U_c',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.b_c = self.add_weight(shape = (self.units,),\n",
    "                                  name = 'b_c',\n",
    "                                  initializer = self.bias_initializer,\n",
    "                                  regularizer = self.bias_regularizer,\n",
    "                                  constraint = self.bias_constraint)\n",
    "             \n",
    "        \"\"\"\"\"\n",
    "            Learnable Weights for the output(o) gate\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        self.V = self.add_weight(shape = (),\n",
    "                                  name = 'V',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.V_ = self.add_weight(shape = (),\n",
    "                                  name = 'V_',\n",
    "                                  initializer = self.recurrent_initializer,\n",
    "                                  regularizer = self.recurrent_regularizer,\n",
    "                                  constraint = self.recurrent_constraint)\n",
    "        \n",
    "        self.b_y = self.add_weight(shape = (),\n",
    "                                  name = 'b_y',\n",
    "                                  initializer = self.bias_initializer,\n",
    "                                  regularizer = self.bias_regularizer,\n",
    "                                  constraint = self.bias_constraint)\n",
    "        \"\"\"\"\n",
    "        *****Documentation example*****\n",
    "        \n",
    "        self.kernel = self.add_weight(name = 'kernel', \n",
    "                                      shape = (input_shape[1], self.output_dim),\n",
    "                                      initializer = 'uniform',\n",
    "                                      trainable = True)\n",
    "        \n",
    "        \"\"\"\"\n",
    "        \n",
    "        self.input_spec = [InputSpec(shape = (self.batch_size, self.timesteps, self.input_dim))]\n",
    "        \n",
    "        super(AttentionGRU, self).build(input_shape)\n",
    "        # OR self.built = True\n",
    "        \n",
    "    def call(self, x):\n",
    "        #layer logic resides here\n",
    "        \n",
    "        #store the entire encoded sequence with the cell.., so that it can be accessed later \n",
    "        self.x_seq = x\n",
    "        \n",
    "        # constant computation\n",
    "        # does not depend on any previous steps\n",
    "        # shape would certainly be the MAXLEN\n",
    "        \n",
    "        self.Wh_hi = _time_distributed_dense(self.x_seq, self.W_h, b = self.b_a, input_dim = ?, timesteps = ?, output_dim = ?)\n",
    "        \n",
    "        return super(AttentionGRU, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        return [y0, s0]\n",
    "        \n",
    "        \n",
    "    def step(self, x, states):\n",
    "        \n",
    "        # most important part of the code\n",
    "        # executes the cell logic\n",
    "        # step is applied to every element of the input sequence\n",
    "        \n",
    "        # need to initialize yt_prev, st_prev\n",
    "        \n",
    "        yt_prev, st_prev = states\n",
    "        \n",
    "        # update gate eq.\n",
    "        z_t = activations.sigmoid(K.dot(yt_prev, self.W_z) + K.dot(yt_prev, self.U_z) + self.b_z)\n",
    "        \n",
    "        # reset gate eq.\n",
    "        r_t = activitions.sigmoid(K.dot(yt_prev, self.W_r) + K.dot(yt_prev, self.U_z) + self.b_r)\n",
    "        \n",
    "        # memory content eq.\n",
    "        c_t = activations.tanh(K.dot(yt_prev, self.W_c) + K.dot((r_t * st_prev), self.U_c) + self.b_c)\n",
    "        \n",
    "        # new hidden state \n",
    "        st = (1 - z_t)*st_prev + z_t*c_t\n",
    "        \n",
    "        st = K.repeat(st, self.timesteps)\n",
    "        \n",
    "        Ws_st = K.dot(st, self.W_s)\n",
    "        \n",
    "        # eq(3)\n",
    "        e_t = K.dot(activations.tanh(Ws_st + self.Wh_hi), K.expand_dims(self.Mu_T))\n",
    "        \n",
    "        # eq(4)\n",
    "        a_t = activations.softmax(et)\n",
    "        \n",
    "        # eq(5)\n",
    "        C = K.squeeze(K.batch_dot(at, self.x_seq, axes = 1), axis = 1)\n",
    "        \n",
    "        # look into the [;] operation\n",
    "        yt = activations.softmax(K.dot(activations.tanh(K.dot(K.concat(st,C),V))),V_)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #specify shape transformation logic here\n",
    "        \n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "    \n",
    "    def get_config():\n",
    "        # allows us to load the model using just a saved file.. once the training is done\n",
    "        # currently.. ignore it\n",
    "        # if possible.. implement later\n",
    "        \n",
    "        config = {\n",
    "            'output_dim' : self.output_dim,\n",
    "            'units' : self.units,\n",
    "            'return_probabilities' : self.return_probabilities\n",
    "        }\n",
    "        \n",
    "        base_config = super(AttentionGRU, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# check if it compiles\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #what's the shape of the input\n",
    "    i = Input(shape = (100,200), dtype = 'float32')\n",
    "    \n",
    "    enc = Bidirectional(LSTM(64, return_sequences=True), merge_mode = 'concat')(i)\n",
    "    dec = AttentionDecoder(32, 4)(enc)\n",
    "    \n",
    "    model = Model(inputs=i, outputs=dec)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "6adccc17bbe02071d921a447ffa4708ed3ad1c6c"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import jsonlines\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import io\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "daad741f9949a185c5278851637f0bab7e7b4c57"
   },
   "outputs": [],
   "source": [
    "# start - <sos> tag\n",
    "# stop - <eos> tag\n",
    "\n",
    "#split records into list of impression, findings\n",
    "def splitReports(File):\n",
    "    \n",
    "    impression_list = []\n",
    "    findings_list = []\n",
    "    \n",
    "    with open(File,'r') as file:\n",
    "        for line in file:\n",
    "            all = line\n",
    "        \n",
    "    report_list = re.findall(r'{.*?}',all)\n",
    "\n",
    "    for report in report_list:\n",
    "        obj = json.loads(report)\n",
    "        # indication_list.append(str(obj['Indication']))\n",
    "        impression_list.append('start ' + str(obj['Impression']) + ' stop')\n",
    "        findings_list.append(str(obj['Findings']))\n",
    "        \n",
    "    return (findings_list, impression_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "ec3a975f695b3a5a82b41014c847cf14cb0461f3"
   },
   "outputs": [],
   "source": [
    "#get vocab of indications, impression, findings\n",
    "def getVocabularySets(findings_list, impression_list):\n",
    "    # indication_vocab_set = set()\n",
    "    impression_vocab_set = set()\n",
    "    findings_vocab_set = set()\n",
    "    common_vocab_set = set()\n",
    "\n",
    "    #for item in indication_list:\n",
    "     #   for word in nltk.word_tokenize(item):\n",
    "      #      indication_vocab_set.add(word.lower())\n",
    "        \n",
    "    for item in impression_list:\n",
    "        for word in nltk.word_tokenize(item):\n",
    "            impression_vocab_set.add(word.lower())\n",
    "        \n",
    "    for item in findings_list:\n",
    "        for word in nltk.word_tokenize(item):\n",
    "            findings_vocab_set.add(word.lower())\n",
    "        \n",
    "    # common_vocab_set.update(indication_vocab_set)\n",
    "    common_vocab_set.update(impression_vocab_set)\n",
    "    common_vocab_set.update(findings_vocab_set)\n",
    "    \n",
    "    return (common_vocab_set, findings_vocab_set, impression_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "7c60d799f7b363eab1f826019eb0282ddc87ee1e"
   },
   "outputs": [],
   "source": [
    "# get the embedding  matrix\n",
    "def loadGloVeModel(gloVe_file):\n",
    "    embedding_model = {}\n",
    "    \n",
    "    with io.open(gloVe_file, encoding = 'utf8') as f:\n",
    "        word_embeddings = f.readlines()\n",
    "        \n",
    "    for word_embedding_line in word_embeddings:\n",
    "        word_embedding = word_embedding_line.split()\n",
    "        word = word_embedding[0]\n",
    "        embedding = np.array([float(col) for col in word_embedding[1:]])\n",
    "        embedding_model[word] = embedding\n",
    "            \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "91e57d58914e00a5b15238d3e60a8225b3fcee17"
   },
   "outputs": [],
   "source": [
    "#get max sequences \n",
    "def getMaxSeq(List):\n",
    "        \n",
    "    max_len = 0\n",
    "    for item in List:\n",
    "        item_len = len(item.split())\n",
    "        if item_len > max_len:\n",
    "            max_len = item_len\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "f22a8169e27f95eca6e1f3ca463883ed6fd2e81a"
   },
   "outputs": [],
   "source": [
    "findings, impression = splitReports('reports.jsonl')\n",
    "common_vocab, findings_vocab, impression_vocab = getVocabularySets(findings, impression)\n",
    "\n",
    "common = []\n",
    "common.extend(findings)\n",
    "#common.extend(indications)\n",
    "common.extend(impression)\n",
    "\n",
    "#max_indication_len = getMaxSeq(indications)\n",
    "max_findings_len = getMaxSeq(findings)\n",
    "max_impression_len = getMaxSeq(impression)\n",
    "\n",
    "MAX_LEN = max(max_findings_len, max_impression_len)\n",
    "MAX_LEN = 100 #99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "dfb99325e47cbe50feb42a8f3ea2c36666c14414"
   },
   "outputs": [],
   "source": [
    "embedding_model = loadGloVeModel('radglove.800M.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "317a3081e58814441368cad34e89ce72228a2c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "e41adc6f0924b77a4c4a9a9f533ad3dae4c4d84c"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(common)\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "\n",
    "#indication_sequences = tokenizer.texts_to_sequences(indications)\n",
    "findings_sequences = tokenizer.texts_to_sequences(findings)\n",
    "impression_sequences = tokenizer.texts_to_sequences(impression)\n",
    "\n",
    "#indication_data = pad_sequences(indication_sequences, maxlen = MAXLEN)\n",
    "findings_data = pad_sequences(findings_sequences, maxlen = MAX_LEN)\n",
    "impression_data = pad_sequences(impression_sequences, maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "fd347110b455a24b64b0532e5818f58b62e5bdc3"
   },
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for word,index in word_to_index.items():\n",
    "    index_to_word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "1c6b605ae1459c0e6bf313391fa6e3909ba888ee"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(common_vocab),EMBEDDING_DIM))\n",
    "\n",
    "for word,index in word_to_index.items():\n",
    "    embedding_vector = embedding_model.get(word)\n",
    "    \n",
    "    if embedding_vector is not None :\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "6eb9910aca58350a97bef6eb153bfcd51ece56bc"
   },
   "outputs": [],
   "source": [
    "#decide maximum sequence length later...\n",
    "MAX_SEQUENCE_LENGTH = MAX_LEN\n",
    "\n",
    "# check doc.s once\n",
    "# trainable=False to prevent the weights from being updated during training\n",
    "embedding_layer = Embedding(len(common_vocab), EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "e95e6c2ae4d6d7f07b5ccaedeb40d19939047f8f"
   },
   "outputs": [],
   "source": [
    "#verify tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "fa26e4a6445b9be3b6684bf2274f031a9223e9bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start Normal chest x-XXXX. stop'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impression[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "16cfffcd6e15616dfb944ef43c9b1628573deb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start normal chest x xxxx stop "
     ]
    }
   ],
   "source": [
    "for i in impression_data[0]:\n",
    "    if i == 0:\n",
    "        continue\n",
    "    print(index_to_word[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "178b2293f0180d62a66a7855094c46ba59f359fd"
   },
   "outputs": [],
   "source": [
    "# this   is     my  life   <eos>\n",
    "\n",
    "# <sos> this    is   my    life   <eos>\n",
    "\n",
    "# ^^^^^ example of teacher forcing\n",
    "\n",
    "#     start -> 8\n",
    "#     stop  -> 9\n",
    "\n",
    "m,_ = findings_data.shape\n",
    "\n",
    "target_data = np.zeros((m, MAX_LEN), dtype = 'int32')\n",
    "\n",
    "#impression_data.shape\n",
    "\n",
    "# offset logic\n",
    "# target_data is nothing but impression_data offset by 1 time-step\n",
    "\n",
    "index = 0\n",
    "for _m in impression_data:\n",
    "    for _t in range(0,MAX_LEN):\n",
    "        if _t > 0 :\n",
    "            target_data[index,_t-1] = impression_data[index,_t]\n",
    "    index = index + 1\n",
    "    \n",
    "#print(impression_data[0])\n",
    "#print(target_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54c802b3fbe68a86adc3fa6917aea4bb8f221d4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e39c093ddefd469d1b507b30ba9ea444d9bbd74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "7850330f0e61a29f8867f17b53da6c3a0694dd78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_uuid": "85a4ac3a6bcdc6b3fec1b1629e5c52acc25a5186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 100)\n",
      "(?, 100, 100)\n",
      "Train on 1808 samples, validate on 453 samples\n",
      "Epoch 1/10\n",
      "1808/1808 [==============================] - 16s 9ms/step - loss: 3808.0808 - val_loss: 2825.5910\n",
      "Epoch 2/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 3302.8663 - val_loss: 2742.4825\n",
      "Epoch 3/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 3125.5390 - val_loss: 2679.3398\n",
      "Epoch 4/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 3032.6299 - val_loss: 2685.8701\n",
      "Epoch 5/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2926.2495 - val_loss: 2633.6648\n",
      "Epoch 6/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2864.2251 - val_loss: 2635.2936\n",
      "Epoch 7/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2843.9562 - val_loss: 2626.0726\n",
      "Epoch 8/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2844.5242 - val_loss: 2652.7529\n",
      "Epoch 9/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2849.9689 - val_loss: 2610.4348\n",
      "Epoch 10/10\n",
      "1808/1808 [==============================] - 8s 4ms/step - loss: 2822.3778 - val_loss: 2567.1428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae740daa10>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Average, Input, GRU, Dense, TimeDistributed, Add, Activation, RepeatVector, Flatten, Permute, Lambda\n",
    "\n",
    "\n",
    "# a ----> attention of findings encoder\n",
    "n_features = 100\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "##ENCODER\n",
    "encoder_inputs = Input(shape = (MAX_LEN,), dtype = 'int32')\n",
    "\n",
    "#print(encoder_inputs.shape)\n",
    "\n",
    "encoder_embeds = embedding_layer(encoder_inputs)\n",
    "\n",
    "#print(encoder_embeds.shape)\n",
    "\n",
    "x1, forward_h = GRU(MAX_LEN, return_sequences = True, return_state = True)(encoder_embeds) #return state is by default True\n",
    "x2, backward_h = GRU(MAX_LEN, return_sequences = True, return_state = True, go_backwards = True)(encoder_embeds)\n",
    "\n",
    "encoder_state = Average()([forward_h, backward_h])\n",
    "X = Average()([x1,x2])\n",
    "\n",
    "#__________#__________#__________#__________#__________#_________#__________#__________#_______________________________________________________\n",
    "\n",
    "##DECODER\n",
    "decoder_inputs = Input(shape = (MAX_LEN,), dtype = 'int32')\n",
    "\n",
    "#print(decoder_inputs.shape)\n",
    "\n",
    "decoder_embeds = embedding_layer(decoder_inputs)\n",
    "\n",
    "#print(decoder_embeds.shape)\n",
    "\n",
    "decoder_outputs,state_s = GRU(100, return_sequences = True, return_state = True)(decoder_embeds, initial_state = encoder_state)\n",
    "\n",
    "Wh_hi = TimeDistributed(Dense(1))(X)\n",
    "\n",
    "Ws_st = TimeDistributed(Dense(1))(decoder_outputs)\n",
    "\n",
    "W = Add()([Wh_hi, Ws_st])\n",
    "\n",
    "e = Dense(1, activation = 'tanh')(W)\n",
    "\n",
    "e  = Flatten()(e)\n",
    "\n",
    "a = Activation('softmax')(e)\n",
    "\n",
    "at = RepeatVector(MAX_LEN)(a)\n",
    "\n",
    "at_ = Permute([2, 1])(at)\n",
    "\n",
    "sent_representation = Multiply()([X, at_])\n",
    "\n",
    "Context = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "\n",
    "term1 = Dense(100)(decoder_outputs)\n",
    "\n",
    "print(term1.shape)\n",
    "\n",
    "term2 = RepeatVector(100)(Context)\n",
    "\n",
    "term2 = Dense(100)(term2)\n",
    "\n",
    "print(term2.shape)\n",
    "\n",
    "weighted_sum = Add()([term1,term2])\n",
    "\n",
    "weighted_sum = Flatten()(weighted_sum)\n",
    "\n",
    "tanh_weighted_sum = Activation('tanh')(weighted_sum)\n",
    "\n",
    "probabilities = Dense(MAX_LEN, activation='softmax')(tanh_weighted_sum)\n",
    "\n",
    "\n",
    "#print(decoder_outputs.shape)\n",
    "\n",
    "#print(Context.shape)\n",
    "\n",
    "#_________#__________#___________#__________#__________#__________#__________#__________#______________________________________________________\n",
    "\n",
    "model = Model(inputs = [encoder_inputs, decoder_inputs], outputs = probabilities)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n",
    "\n",
    "model.fit([findings_data, impression_data], target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a277c5e524f9fa8714c290f947ef1f221d378c03"
   },
   "outputs": [],
   "source": [
    "embedding_matrix[findings_data].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf8e3f6b7c1140802bcdfea40171f9f541d8fa70"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

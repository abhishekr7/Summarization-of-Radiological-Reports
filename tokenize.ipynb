{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import jsonlines\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import io\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ - <sos> tag\n",
    "# $ - <eos> tag\n",
    "\n",
    "#split records into list of indications, impression, findings\n",
    "def splitReports(File):\n",
    "    indication_list = []\n",
    "    impression_list = []\n",
    "    findings_list = []\n",
    "    \n",
    "    with open(File,'r') as file:\n",
    "        for line in file:\n",
    "            all = line\n",
    "        \n",
    "    report_list = re.findall(r'{.*?}',all)\n",
    "\n",
    "    for report in report_list:\n",
    "        obj = json.loads(report)\n",
    "        indication_list.append(str(obj['Indication']))\n",
    "        impression_list.append(str(obj['Impression']))\n",
    "        findings_list.append(str(obj['Findings']))\n",
    "        \n",
    "    return (indication_list, findings_list, impression_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocab of indications, impression, findings\n",
    "def getVocabularySets(indication_list, findings_list, impression_list):\n",
    "    indication_vocab_set = set()\n",
    "    impression_vocab_set = set()\n",
    "    findings_vocab_set = set()\n",
    "    common_vocab_set = set()\n",
    "\n",
    "    for item in indication_list:\n",
    "        for word in nltk.word_tokenize(item):\n",
    "            indication_vocab_set.add(word.lower())\n",
    "        \n",
    "    for item in impression_list:\n",
    "        for word in nltk.word_tokenize(item):\n",
    "            impression_vocab_set.add(word.lower())\n",
    "        \n",
    "    for item in findings_list:\n",
    "        for word in nltk.word_tokenize(item):\n",
    "            findings_vocab_set.add(word.lower())\n",
    "        \n",
    "    common_vocab_set.update(indication_vocab_set)\n",
    "    common_vocab_set.update(impression_vocab_set)\n",
    "    common_vocab_set.update(findings_vocab_set)\n",
    "    \n",
    "    return (common_vocab_set, indication_vocab_set, findings_vocab_set, impression_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding  matrix\n",
    "def loadGloVeModel(gloVe_file):\n",
    "    embedding_model = {}\n",
    "    \n",
    "    with io.open(gloVe_file, encoding = 'utf8') as f:\n",
    "        word_embeddings = f.readlines()\n",
    "        \n",
    "    for word_embedding_line in word_embeddings:\n",
    "        word_embedding = word_embedding_line.split()\n",
    "        word = word_embedding[0]\n",
    "        embedding = np.array([float(col) for col in word_embedding[1:]])\n",
    "        embedding_model[word] = embedding\n",
    "            \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max sequences \n",
    "def getMaxSeq(List):\n",
    "        \n",
    "    max_len = 0\n",
    "    for item in List:\n",
    "        if len(item) > max_len:\n",
    "            max_len = len(item)\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "indications, findings, impressions = splitReports('reports_1.jsonl')\n",
    "common_vocab, indication_vocab, findings_vocab, impression_vocab = getVocabularySets(indications, impressions, findings)\n",
    "\n",
    "common = []\n",
    "common.extend(findings)\n",
    "common.extend(indications)\n",
    "common.extend(impressions)\n",
    "\n",
    "max_indication_len = getMaxSeq(indications)\n",
    "max_findings_len = getMaxSeq(findings)\n",
    "max_impression_len = getMaxSeq(impressions)\n",
    "\n",
    "MAXLEN = max(max_indication_len, max_findings_len, max_impression_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = loadGloVeModel('radglove.800M.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding dimension\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(common)\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "\n",
    "indication_sequences = tokenizer.texts_to_sequences(indications)\n",
    "findings_sequences = tokenizer.texts_to_sequences(findings)\n",
    "impression_sequences = tokenizer.texts_to_sequences(impressions)\n",
    "\n",
    "indication_data = pad_sequences(indication_sequences, maxlen = MAXLEN)\n",
    "findings_data = pad_sequences(findings_sequences, maxlen = MAXLEN)\n",
    "impression_data = pad_sequences(impression_sequences, maxlen = MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for word,index in word_to_index.items():\n",
    "    index_to_word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(common_vocab),EMBEDDING_DIM))\n",
    "\n",
    "for word,index in word_to_index.items():\n",
    "    embedding_vector = embedding_model.get(word)\n",
    "    \n",
    "    if embedding_vector is not None :\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide maximum sequence length later...\n",
    "MAX_SEQUENCE_LENGTH = MAXLEN\n",
    "\n",
    "# check doc.s once\n",
    "# trainable=False to prevent the weights from being updated during training\n",
    "embedding_layer = Embedding(len(common_vocab), EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cardiac silhouette and mediastinum size are within normal limits there is no pulmonary edema there is no focal consolidation there are no xxxx of a pleural effusion there is no evidence of pneumothorax "
     ]
    }
   ],
   "source": [
    "for i in findings_data[0]:\n",
    "    if i == 0:\n",
    "        continue\n",
    "    print(index_to_word[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "# a ----> attention of findings encoder\n",
    "# a' ---> attention of background encoder\n",
    "\n",
    "encoder1_inputs = Input( input_shape, dtype = 'int32')\n",
    "\n",
    "x = embedding_layer(encoder1_inputs)\n",
    "\n",
    "x, state_h, state_c = Bidirectional(LSTM(100, return_state = True))(x) #return state is by default True\n",
    "\n",
    "encoder_state = state_h\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "encoder2_inputs = Input( input_shape, dtype = 'int32')\n",
    "\n",
    "x = embedding_layer(encoder2_inputs)\n",
    "\n",
    "x, state_h, state_c = Bidirectional(LSTM(100, return_state = True))()\n",
    "\n",
    "decoder_inputs = Input( input_shape, dtype = 'int32')\n",
    "\n",
    "_ = embedding_layer(decoder_inputs)\n",
    "\n",
    "_ = LSTM(200, return_sequences = True)(_ ,initial_state = encoder_states)\n",
    "\n",
    "decoder_outputs = Dense()_\n",
    "\n",
    "model = Model([encoder1_inputs, encoder2_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n",
    "\n",
    "model.fit([indication_data ,findings_data, impression_data], impression_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4363995844d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_2_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
